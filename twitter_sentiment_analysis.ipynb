{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "twitter_sentiment_analysis.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yveFAg68umBV",
        "outputId": "a9646e5b-b079-444b-e4b7-21a19fd7a312"
      },
      "source": [
        "#importing modules\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import regex as re\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I2zmdV_fuvQG",
        "outputId": "be6604d7-9829-4789-e89a-201695686cd6"
      },
      "source": [
        "#importing dataset\n",
        "df = pd.read_csv(\"/content/data.csv\", encoding=\"ISO-8859-1\")\n",
        "print(df.columns)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Index(['sentiment', 'text', 'user'], dtype='object')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WE9Ji6udu11t"
      },
      "source": [
        "#dropping useless columns\n",
        "df = df.drop([\"user\"], axis=1)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xg3EWWpVu9Ne",
        "outputId": "e603bd58-13a9-4011-8f9f-be48a786ed09"
      },
      "source": [
        "#get the unique values of sentiment column\n",
        "print(df[\"sentiment\"].unique())"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['positive' 'neutral' 'negative']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YHHy9UIJwacx",
        "outputId": "9a8ee3a2-cf69-4f5c-c385-a82714e39456"
      },
      "source": [
        "#get the probability of a tweet being positive, negative or neutral\n",
        "prob_positive = len([tweet for tweet in df[\"sentiment\"] if tweet == 'positive']) / len(df)\n",
        "prob_neutral = len([tweet for tweet in df[\"sentiment\"] if tweet == 'neutral']) / len(df)\n",
        "prob_negative = len([tweet for tweet in df[\"sentiment\"] if tweet == 'negative']) / len(df)\n",
        "print(prob_positive, prob_negative, prob_neutral)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.4069767441860465 0.3488372093023256 0.2441860465116279\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b8lJJGCrwjpZ",
        "outputId": "7bbbd599-6fd8-4ec2-ad09-0393fc084cae"
      },
      "source": [
        "#clean the tweets by removing the hashtags, mentions, non-english alphabets and other links\n",
        "texts = []\n",
        "for text in df[\"text\"]:\n",
        "  text = re.sub(r\"(^RT\\s+@.*:\\s)\", \"\", text)\n",
        "  text = re.sub(r\"https:.*\", \"\", text)\n",
        "  text = re.sub(r\"(\\s)[#@]+\\w+\", \"\", text) #removing all mentions and hashtags that occur in middle of tweets\n",
        "  text = re.sub(r\"([#@]+\\w+\\s)\", \"\", text) #removing all mentions and hashtags that occur at the starting of tweets\n",
        "  text = re.sub(r\"[,.?!]*\", \"\", text)\n",
        "  text = re.sub(r\"[^a-zA-Z\\s']+\", \"\", text) #removing words that contain characters other than english alphabets and spaces.\n",
        "  texts.append(text)\n",
        "df[\"text\"] = texts\n",
        "print(texts)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['The incredible reactions as Brandon Moreno became the first Mexicanborn champion in UFC history  ', 'CAN ANYBODY HEAR ME ', 'Guys how do u know she is a good rider ', 'We want to justice ', 'Farmers in Tsp Sagaing Division staged a sesame harvest strike to overthrow the military dictatorship  ', 'We are not Hindus Separate vellalars from hindu nWe are the real Vellalars ', \"Saturn's icy moon Enceladus captured by the Cassini spacecraft The moon's surface is covered with fractures folds\", 'And here is some more local news on power reconnections ', 'MT Studio said that back in LA to prepare for his new songcontent ', 'i wish one day you could see yourself with my eyes  ', 'Comedian Atif Muhammad Aziz jokes on Indian Army Ram Lala Cow meatnnNothing hilarious to make anyone laugh Audience', '', 'Someone in the product team is in a long distance relationship', 'what if I made you heart shaped gorditas aha unless ', 'Classic Tim ', \"Today is The Queen's Official Birthday A military parade held by is taking place in the Quadrangle of \", 'Found  puppies on the side of the road today they need a loving home A simple retweet can find these babies a home', \"Alright do you have anything you wanted to do with your father before ' \", 'I already got  still short of  please keep boosting', 'ie Boris JOHNSON is coming out as a RACIST and a COWARD ', 'Verum Rex  Kingdom Hearts fans ARE YOU READY  ', 'Probably pushing away good person ', \"Baba you day yarn well God's blessings shall always follow this young man \", 'That clown is really angry n', 'The weather is great today', 'This dude is awesome and talented and super sweet ', 'The cake is very tasty', 'What a lovely dress', 'Excellent performance by Justin Beiber tonight', 'This video game is very exciting and awesome ', 'A day well spent Looking forward to next outing', 'Marvellous showcase by the band Rooting for them all the way', 'Well played champ success awaiting you', 'This dude saved a child from a car accident he is a hero', 'The road is being constructed anew and will connect to many new places', \"The movie was surreal Can't wait for the sequel furious\", 'A standing ovation for the great Sachin Tendulkar with cheers all around', ' Wonderful getsure by the great Kapil Dev at the sports award ceremony', 'Fascinating scenes at the fireworks station mumbai', 'I like playing guitar whenever i get the time to', 'Love the wonderful and sweet personality of Rohit', 'The wonderful day continues', 'In three years everyone will be happy', 'She loves to write short stories in the local coffee shop', 'They are very caring and loving towards him', 'Think positive and stay fit', 'We will surely win this time', 'Lets have this party started', 'I love classical music', 'I hate when holidays are over', 'This drink is sour and bitter', \"I don't want to go to this place ever again\", \"This couldn't have ended any more worse\", 'Another wasted year thanks to covid', 'Disappointing performance by the boys', 'Donal Trump is an idiot', 'Get the hell out of here', 'This sport sucks', 'Why this fool is still in the team', 'How can a movie this boring break this many records', 'Fatima is a liar', 'I hate rock music', 'Someone get me out of this hell', 'This annoying brat keeps on following me everywhere', 'She did not like Bikhram yoga', 'They are late for the party again', 'I just need to be alone', 'These past few months have been dipressing and disappointing', \"I can gurantee he'll lose again\", \"They don't visit the old man anymore\", \"I can't stand being ignored for this long\", 'All that exercising left me weak and tired', 'We broke up last week', 'My childhood got destroyed by this incident', 'Is this the right way to Dehradun', 'hello everyone', 'Just moved to a new place', 'Anyone heard of programming', 'Just found my teenage diary from the basement', \"Maybe I'll walk to work from tommorow onwards\", 'Can i start learning something at this age', 'She left for Mumbai  days ago', 'Boys will be arriving back in India today', 'Where did the cats go', 'Who lets the dogs out', 'This place is nostalgic']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ye6mY-PXwzbb"
      },
      "source": [
        "#merge the positive tweets together. Do the same for negative and neutral tweets also.\n",
        "positive_tweets_list = []\n",
        "negative_tweets_list = []\n",
        "neutral_tweets_list = []\n",
        "for (label, tweet) in zip(df[\"sentiment\"], df[\"text\"]):\n",
        "  if label == 'positive':\n",
        "    positive_tweets_list.append(tweet)\n",
        "  elif label == 'negative':\n",
        "    negative_tweets_list.append(tweet)\n",
        "  else:\n",
        "    neutral_tweets_list.append(tweet)\n",
        "\n",
        "positive_tweets = \" \".join(positive_tweets_list)\n",
        "negative_tweets = \" \".join(negative_tweets_list)\n",
        "neutral_tweets = \" \".join(neutral_tweets_list)\n",
        "\n",
        "#Create a string that contains all the tweets too.\n",
        "all_tweets = positive_tweets + \" \" + negative_tweets + \" \" + neutral_tweets"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1QSgVq2BxES4"
      },
      "source": [
        "#define a function that returns the bag of words while removing stopwords and stemming the words as well.\n",
        "#bag of words is dictionary where each word is stored with the no of times it appears in the text.\n",
        "def bag_of_words(sentence):\n",
        "  bag = {}\n",
        "  ps = PorterStemmer()\n",
        "  #sw = stopwords.words('english')\n",
        "  for word in sentence.lower().split():\n",
        "    #word = ps.stem(word)\n",
        "    #if word in sw:\n",
        "      #pass\n",
        "    if word in bag:\n",
        "      bag[word] += 1\n",
        "    else:\n",
        "      bag[word] = 1\n",
        "  return bag"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49ougcaixZLT"
      },
      "source": [
        "#create a bag of words for the negative, positive and neutral tweets separately.\n",
        "negative_words = bag_of_words(negative_tweets)\n",
        "positive_words = bag_of_words(positive_tweets)\n",
        "neutral_words = bag_of_words(neutral_tweets)\n",
        "\n",
        "#create a bag of words for all tweets combined too.\n",
        "all_words = bag_of_words(all_tweets)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SQHGPeeoS9Tl",
        "outputId": "71064d0b-f988-419d-aa83-cc8ae4b102a2"
      },
      "source": [
        "print(\"positive_words: \", positive_words)\n",
        "print(\"negative_words: \", negative_words)\n",
        "print(\"neutral_words: \", neutral_words)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "positive_words:  {'the': 22, 'incredible': 1, 'reactions': 1, 'as': 1, 'brandon': 1, 'moreno': 1, 'became': 1, 'first': 1, 'mexicanborn': 1, 'champion': 1, 'in': 5, 'ufc': 1, 'history': 1, 'guys': 1, 'how': 1, 'do': 1, 'u': 1, 'know': 1, 'she': 2, 'is': 10, 'a': 11, 'good': 1, 'rider': 1, 'we': 2, 'are': 4, 'not': 1, 'hindus': 1, 'separate': 1, 'vellalars': 2, 'from': 2, 'hindu': 1, 'nwe': 1, 'real': 1, 'and': 8, 'here': 1, 'some': 1, 'more': 1, 'local': 2, 'news': 1, 'on': 2, 'power': 1, 'reconnections': 1, 'mt': 1, 'studio': 1, 'said': 1, 'that': 1, 'back': 1, 'la': 1, 'to': 5, 'prepare': 1, 'for': 4, 'his': 1, 'new': 2, 'songcontent': 1, 'classic': 1, 'tim': 1, 'today': 3, \"queen's\": 1, 'official': 1, 'birthday': 1, 'military': 1, 'parade': 1, 'held': 1, 'by': 4, 'taking': 1, 'place': 1, 'quadrangle': 1, 'of': 3, 'found': 1, 'puppies': 1, 'side': 1, 'road': 2, 'they': 2, 'need': 1, 'loving': 2, 'home': 2, 'simple': 1, 'retweet': 1, 'can': 1, 'find': 1, 'these': 1, 'babies': 1, 'verum': 1, 'rex': 1, 'kingdom': 1, 'hearts': 1, 'fans': 1, 'you': 3, 'ready': 1, 'baba': 1, 'day': 3, 'yarn': 1, 'well': 3, \"god's\": 1, 'blessings': 1, 'shall': 1, 'always': 1, 'follow': 1, 'this': 6, 'young': 1, 'man': 1, 'weather': 1, 'great': 3, 'dude': 2, 'awesome': 2, 'talented': 1, 'super': 1, 'sweet': 2, 'cake': 1, 'very': 3, 'tasty': 1, 'what': 1, 'lovely': 1, 'dress': 1, 'excellent': 1, 'performance': 1, 'justin': 1, 'beiber': 1, 'tonight': 1, 'video': 1, 'game': 1, 'exciting': 1, 'spent': 1, 'looking': 1, 'forward': 1, 'next': 1, 'outing': 1, 'marvellous': 1, 'showcase': 1, 'band': 1, 'rooting': 1, 'them': 1, 'all': 2, 'way': 1, 'played': 1, 'champ': 1, 'success': 1, 'awaiting': 1, 'saved': 1, 'child': 1, 'car': 1, 'accident': 1, 'he': 1, 'hero': 1, 'being': 1, 'constructed': 1, 'anew': 1, 'will': 3, 'connect': 1, 'many': 1, 'places': 1, 'movie': 1, 'was': 1, 'surreal': 1, \"can't\": 1, 'wait': 1, 'sequel': 1, 'furious': 1, 'standing': 1, 'ovation': 1, 'sachin': 1, 'tendulkar': 1, 'with': 1, 'cheers': 1, 'around': 1, 'wonderful': 3, 'getsure': 1, 'kapil': 1, 'dev': 1, 'at': 2, 'sports': 1, 'award': 1, 'ceremony': 1, 'fascinating': 1, 'scenes': 1, 'fireworks': 1, 'station': 1, 'mumbai': 1, 'i': 3, 'like': 1, 'playing': 1, 'guitar': 1, 'whenever': 1, 'get': 1, 'time': 2, 'love': 2, 'personality': 1, 'rohit': 1, 'continues': 1, 'three': 1, 'years': 1, 'everyone': 1, 'be': 1, 'happy': 1, 'loves': 1, 'write': 1, 'short': 1, 'stories': 1, 'coffee': 1, 'shop': 1, 'caring': 1, 'towards': 1, 'him': 1, 'think': 1, 'positive': 1, 'stay': 1, 'fit': 1, 'surely': 1, 'win': 1, 'lets': 1, 'have': 1, 'party': 1, 'started': 1, 'classical': 1, 'music': 1}\n",
            "negative_words:  {'farmers': 1, 'in': 2, 'tsp': 1, 'sagaing': 1, 'division': 1, 'staged': 1, 'a': 5, 'sesame': 1, 'harvest': 1, 'strike': 1, 'to': 6, 'overthrow': 1, 'the': 6, 'military': 1, 'dictatorship': 1, 'comedian': 1, 'atif': 1, 'muhammad': 1, 'aziz': 1, 'jokes': 1, 'on': 2, 'indian': 1, 'army': 1, 'ram': 1, 'lala': 1, 'cow': 1, 'meatnnnothing': 1, 'hilarious': 1, 'make': 1, 'anyone': 1, 'laugh': 1, 'audience': 1, 'ie': 1, 'boris': 1, 'johnson': 1, 'is': 6, 'coming': 1, 'out': 3, 'as': 1, 'racist': 1, 'and': 4, 'coward': 1, 'probably': 1, 'pushing': 1, 'away': 1, 'good': 1, 'person': 1, 'that': 2, 'clown': 1, 'really': 1, 'angry': 1, 'n': 1, 'i': 6, 'hate': 2, 'when': 1, 'holidays': 1, 'are': 2, 'over': 1, 'this': 11, 'drink': 1, 'sour': 1, 'bitter': 1, \"don't\": 2, 'want': 1, 'go': 1, 'place': 1, 'ever': 1, 'again': 3, \"couldn't\": 1, 'have': 2, 'ended': 1, 'any': 1, 'more': 1, 'worse': 1, 'another': 1, 'wasted': 1, 'year': 1, 'thanks': 1, 'covid': 1, 'disappointing': 2, 'performance': 1, 'by': 2, 'boys': 1, 'donal': 1, 'trump': 1, 'an': 1, 'idiot': 1, 'get': 2, 'hell': 2, 'of': 2, 'here': 1, 'sport': 1, 'sucks': 1, 'why': 1, 'fool': 1, 'still': 1, 'team': 1, 'how': 1, 'can': 2, 'movie': 1, 'boring': 1, 'break': 1, 'many': 1, 'records': 1, 'fatima': 1, 'liar': 1, 'rock': 1, 'music': 1, 'someone': 1, 'me': 3, 'annoying': 1, 'brat': 1, 'keeps': 1, 'following': 1, 'everywhere': 1, 'she': 1, 'did': 1, 'not': 1, 'like': 1, 'bikhram': 1, 'yoga': 1, 'they': 2, 'late': 1, 'for': 2, 'party': 1, 'just': 1, 'need': 1, 'be': 1, 'alone': 1, 'these': 1, 'past': 1, 'few': 1, 'months': 1, 'been': 1, 'dipressing': 1, 'gurantee': 1, \"he'll\": 1, 'lose': 1, 'visit': 1, 'old': 1, 'man': 1, 'anymore': 1, \"can't\": 1, 'stand': 1, 'being': 1, 'ignored': 1, 'long': 1, 'all': 1, 'exercising': 1, 'left': 1, 'weak': 1, 'tired': 1, 'we': 1, 'broke': 1, 'up': 1, 'last': 1, 'week': 1, 'my': 1, 'childhood': 1, 'got': 1, 'destroyed': 1, 'incident': 1}\n",
            "neutral_words:  {'can': 2, 'anybody': 1, 'hear': 1, 'me': 1, 'we': 1, 'want': 1, 'to': 5, 'justice': 1, \"saturn's\": 1, 'icy': 1, 'moon': 1, 'enceladus': 1, 'captured': 1, 'by': 1, 'the': 7, 'cassini': 1, 'spacecraft': 1, \"moon's\": 1, 'surface': 1, 'is': 4, 'covered': 1, 'with': 3, 'fractures': 1, 'folds': 1, 'i': 4, 'wish': 1, 'one': 1, 'day': 1, 'you': 4, 'could': 1, 'see': 1, 'yourself': 1, 'my': 2, 'eyes': 1, 'someone': 1, 'in': 3, 'product': 1, 'team': 1, 'a': 2, 'long': 1, 'distance': 1, 'relationship': 1, 'what': 1, 'if': 1, 'made': 1, 'heart': 1, 'shaped': 1, 'gorditas': 1, 'aha': 1, 'unless': 1, 'alright': 1, 'do': 2, 'have': 1, 'anything': 1, 'wanted': 1, 'your': 1, 'father': 1, 'before': 1, \"'\": 1, 'already': 1, 'got': 1, 'still': 1, 'short': 1, 'of': 2, 'please': 1, 'keep': 1, 'boosting': 1, 'this': 3, 'right': 1, 'way': 1, 'dehradun': 1, 'hello': 1, 'everyone': 1, 'just': 2, 'moved': 1, 'new': 1, 'place': 2, 'anyone': 1, 'heard': 1, 'programming': 1, 'found': 1, 'teenage': 1, 'diary': 1, 'from': 2, 'basement': 1, 'maybe': 1, \"i'll\": 1, 'walk': 1, 'work': 1, 'tommorow': 1, 'onwards': 1, 'start': 1, 'learning': 1, 'something': 1, 'at': 1, 'age': 1, 'she': 1, 'left': 1, 'for': 1, 'mumbai': 1, 'days': 1, 'ago': 1, 'boys': 1, 'will': 1, 'be': 1, 'arriving': 1, 'back': 1, 'india': 1, 'today': 1, 'where': 1, 'did': 1, 'cats': 1, 'go': 1, 'who': 1, 'lets': 1, 'dogs': 1, 'out': 1, 'nostalgic': 1}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CpjFege7xwBM"
      },
      "source": [
        "#predict the sentiment by calculating probabilities for each sentiment using naive bayes formula\n",
        "def predict(sentence):\n",
        "  positive_prob = 1\n",
        "  negative_prob = 1\n",
        "  neutral_prob = 1\n",
        "  for word in sent.split():\n",
        "\n",
        "    if word in positive_words:\n",
        "      prob_word_is_positive = (positive_words[word] + 1)/ (sum(positive_words.values()) + 1) \n",
        "    else:\n",
        "      prob_word_is_positive = 1 / (sum(positive_words.values()) +1)\n",
        "\n",
        "    if word in all_words:\n",
        "      prob_word = (all_words[word] + 1) / (sum(all_words.values()) +1) \n",
        "    else:\n",
        "      prob_word = 1 / (sum(all_words.values()) +1)\n",
        "\n",
        "    if word in negative_words:\n",
        "      prob_word_is_negative = (negative_words[word] + 1) / (sum(negative_words.values()) +1)\n",
        "    else:\n",
        "      prob_word_is_negative = 1 / (sum(negative_words.values()) +1)\n",
        "\n",
        "    if word in negative_words:\n",
        "      prob_word_is_negative = (negative_words[word] + 1) / (sum(negative_words.values()) +1)\n",
        "    else:\n",
        "      prob_word_is_negative = 1 / (sum(negative_words.values()) +1)\n",
        "\n",
        "    if word in neutral_words:\n",
        "      prob_word_is_neutral = (neutral_words[word] + 1) / (sum(neutral_words.values()) +1)\n",
        "    else:\n",
        "      prob_word_is_neutral = 1 / (sum(neutral_words.values()) +1)\n",
        "\n",
        "    positive_prob *= prob_word_is_positive * prob_positive / prob_word\n",
        "    negative_prob *= prob_word_is_negative * prob_negative / prob_word\n",
        "    neutral_prob *= prob_word_is_neutral * prob_neutral / prob_word\n",
        "    print(\"positive: {0:.2f}%\".format(positive_prob*100))\n",
        "    print(\"negative: {0:.2f}%\".format(negative_prob*100))\n",
        "    print(\"neutral: {0:.2f}%\".format(neutral_prob*100))\n",
        "    if neutral_prob > 1:\n",
        "      return \"neutral\"\n",
        "    elif positive_prob >= negative_prob:\n",
        "      return \"positive\"\n",
        "    else:\n",
        "      return \"negative\""
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x_KqM_UuxmH1",
        "outputId": "b876084a-fca6-4467-daa9-e4eb0bed6834"
      },
      "source": [
        "#predicting probabilities of tweet being positive or negative\n",
        "while True:\n",
        "  sent = input(\"Enter a sentence: \")\n",
        "  if sent.lower()==\"bye\":\n",
        "    break\n",
        "  result = predict(sent)\n",
        "  print(\"Sentence is: \", result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter a sentence: hello\n",
            "positive: 43.72%\n",
            "negative: 54.77%\n",
            "neutral: 111.49%\n",
            "Sentence is:  neutral\n",
            "Enter a sentence: this game is annoying\n",
            "positive: 29.15%\n",
            "negative: 62.60%\n",
            "neutral: 21.24%\n",
            "Sentence is:  negative\n",
            "Enter a sentence: sachin is a legendary cricketer\n",
            "positive: 87.44%\n",
            "negative: 54.77%\n",
            "neutral: 55.75%\n",
            "Sentence is:  positive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RF0P6lJ3yYnc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}